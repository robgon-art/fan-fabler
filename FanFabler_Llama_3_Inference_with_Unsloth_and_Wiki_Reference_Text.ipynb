{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/fan-fabler/blob/main/FanFabler_Llama_3_Inference_with_Unsloth_and_Wiki_Reference_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV_qanpPIw5H",
        "outputId": "f22deeac-086f-458d-a70d-8ba4396cc99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 30 20:03:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   49C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAjd8PwEMKFb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "a9cadc75-3873-4a13-bfc9-e2912cdaed2f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"robgonsalves/llama-3-8b-Instruct-bnb-4bit-lora-64-64-lora\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def get_snippets(wp_code, subject, limit=5):\n",
        "    search_url = f\"https://{wp_code}.wikipedia.org/w/api.php\"\n",
        "    search_params = {\n",
        "        \"action\": \"opensearch\",\n",
        "        \"search\": subject,\n",
        "        \"limit\": str(limit),\n",
        "        \"namespace\": \"0\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    search_response = requests.get(search_url, params=search_params)\n",
        "    search_data = search_response.json()\n",
        "\n",
        "    titles = search_data[1]\n",
        "    results = []\n",
        "    for title in titles:\n",
        "        extracts_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts\",\n",
        "            \"exsentences\": 20,  # Estimate for 500 words\n",
        "            \"titles\": title,\n",
        "            \"explaintext\": True,\n",
        "        }\n",
        "\n",
        "        extracts_response = requests.get(search_url, params=extracts_params)\n",
        "        extracts_data = extracts_response.json()\n",
        "\n",
        "        pages = extracts_data['query']['pages']\n",
        "        for page_id in pages:\n",
        "            page = pages[page_id]\n",
        "            snippet = page.get('extract', 'Content not available.').replace('\\n', ' ')\n",
        "            results.append({\"title\": title, \"snippet\": snippet})\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Bf_iyMi7rRim",
        "outputId": "5ada6dcc-3ad6-4f23-d7cb-4fde32fe9ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"Respond to the user's query based on the conversation and the context.\n",
        "Respond in the same language as the user's query.\n",
        "To get more from context from the Wikipedia, indicate a Wiki page and language code at the end of the repsonse using this format:\n",
        ">>>search term\n",
        ">>>language code\"\"\"\n",
        "\n",
        "single_message_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{}<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "{}<|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mu7JMiE-nmwX",
        "outputId": "0c5908dd-92d0-4a7d-976c-801cf0e64e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_question = \"List three ideas for writing fan fiction about Curb Your Enthusiasm.\"\n",
        "\n",
        "text = single_message_prompt.format(system_prompt, first_question)\n",
        "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "resulting_tokens = model.generate(**inputs, streamer = text_streamer,\n",
        "  max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id, do_sample = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "NEF10_TqzGCa",
        "outputId": "6726fff2-df06-48eb-b837-f7c664fe9bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are three ideas for writing fan fiction about Curb Your Enthusiasm:\n",
            "\n",
            "1. A story centered around Larry David's interactions with a new character who is a die-hard fan of the show, creating humorous misunderstandings and awkward situations.\n",
            "2. Exploring what would happen if Larry found himself on the receiving end of a misunderstanding similar to those he causes, and navigating the consequences of being the one at the center of chaos.\n",
            "3. A romantic fan fiction story focusing on Larry's relationships, perhaps involving a new love interest that challenges his social norms and creates unexpected romantic misadventures.\n",
            "\n",
            ">>>Curb Your Enthusiasm\n",
            ">>>en<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resulting_text = tokenizer.batch_decode(resulting_tokens)\n",
        "results_list = resulting_text[0].split(\"<|end_header_id|>\")\n",
        "first_answer = results_list[-1].replace(\"<|eot_id|>\", \"\")\n",
        "# print(first_answer)\n",
        "results_list = first_answer.split(\">>>\")\n",
        "search_term = results_list[-2].replace(\"search term:\", \"\").strip()\n",
        "language_code = results_list[-1].replace(\"language code:\", \"\").strip()\n",
        "print(search_term, \" - \", language_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PwTdwLJpvD96",
        "outputId": "01a68d85-5816-4f8e-d688-f4b1161662bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curb Your Enthusiasm  -  en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supporting_text = \"\"\n",
        "\n",
        "if search_term and language_code:\n",
        "  supporting_text_array = get_snippets(language_code, search_term)\n",
        "  if len(supporting_text_array) > 1:\n",
        "    # print(supporting_text_array)\n",
        "    supporting_text = supporting_text_array[0][\"snippet\"]\n",
        "    support_text = supporting_text_array[0]\n",
        "    supporting_text = supporting_text.replace(\"\\n\", \" \")\n",
        "\n",
        "print(supporting_text)"
      ],
      "metadata": {
        "id": "eJnHokoKy3bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "6IB6ssqvpCdk",
        "outputId": "3ac0e522-8e38-43da-b2a2-3658140ecb97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Talent Show Takedown is a fan fiction episode of Curb Your Enthusiasm, where Larry David gets involved in a school talent show for his children's benefit. The story revolves around Larry trying to outdo the other parents by using a high-tech video production for his son's act, which leads to a series of mishaps and misunderstandings. As Larry navigates the competitive atmosphere and his own ego, the episode becomes a hilarious exploration of social awkwardness, personal growth, and fatherly love.  Characters involved:  1. Larry David - The main character who gets caught up in the talent show and tries to outshine the other parents. 2. Cheryl - Larry's wife who tries to reason with him and prevent the chaos from unfolding. 3. Jeff Greene - Larry's friend and manager who offers unsolicited advice and meddling in the situation. 4. The other parents - Various characters who participate in the talent show and clash with Larry.  This episode delves into the themes of family dynamics, ambition, and the importance of humility, while also showcasing the signature humor and wit of the Curb Your Enthusiasm universe.  Let me know when you're ready to start writing, and I'll be happy to help you brainstorm further details!<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "second_question = \"Write an outline for The Talent Show Takedown\"\n",
        "\n",
        "long_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "{}\"\"\"\n",
        "\n",
        "text = long_prompt.format(system_prompt, first_question, first_answer, supporting_text, second_question, \"\")\n",
        "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "resulting_tokens = model.generate(**inputs, streamer = text_streamer,\n",
        "  max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id, do_sample = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Citez trois idées pour écrire une fanfiction sur Amélie.\"\n",
        "text = single_message_prompt.format(system_prompt, prompt)\n",
        "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "resulting_tokens = model.generate(**inputs, streamer = text_streamer,\n",
        "  max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id, do_sample = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "E-TUS8BOWn98",
        "outputId": "93b1197e-8413-47da-c8bf-524c29ad9941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bien sûr! Voici trois idées pour écrire une fanfiction sur Amélie:\n",
            "1. Amélie tombe amoureuse d'un mystérieux personnage introduit dans le film. Ils commencent à se fréquenter en secret, et Amélie doit faire face aux défis de l'amour tout en gardant son identité cachée.\n",
            "2. Alors que Nino quitte Amélie, elle se retrouve dévastée. Son fanfiction pourrait explorer le déroulement de ce départ, et son influence sur Amélie qui essaye de continuer à faire des bonheurs sans Nino.\n",
            "3. Amélie se retrouve impliquée dans une intrigue intrigante où elle doit relever un défi particulier pour sauver un personnage aimé. Ce défi développerait ses compétences et sa résilience pour accomplir son premier bonheur à elle-même.\n",
            "\n",
            ">>>Amélie\n",
            ">>>fr<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"방탄소년단에 관한 팬픽션 아이디어 3가지를 보여주세요.\"\n",
        "text = single_message_prompt.format(system_prompt, prompt)\n",
        "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "resulting_tokens = model.generate(**inputs, streamer = text_streamer,\n",
        "  max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id, do_sample = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ZwtSsEN4Rfq9",
        "outputId": "ae281341-4bcc-453f-ddc9-be431aba1197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "방탄소년단에 관한 팬픽션 아이디어를 몇 가지 제시해 드리겠습니다. \n",
            "\n",
            "1. 미스터 션샤인 TV 드라마와의 차가운 팀업 스토리: 방탄소년단 멤버들이 드라마 '미스터 션샤인' 속의 캐릭터들과 팀업하여 무대에서 함께 노래하는 판타지 스토리입니다. \n",
            "\n",
            "2. 영화 '히어로 프로젝트' 속에 오브젝트를 중심으로하는 Fan Fiction: 방탄소년단 멤버들이 히어로 프로젝트 세계에서 등장하며 각자의 모습을 보여주는 다양한 스토리들을 중심으로 한 Fan Fiction를 만드는 것입니다.\n",
            "\n",
            "3. '이방인' 소설의 패러디: 방탄소년단 멤버들이 '이방인' 소설의 세계 속에서 대학교를 다니는 학생으로 등장하면서 어떻게 펼쳐질 것인지를 상상해 보는 것입니다.\n",
            "\n",
            "이러한 아이디어를 바탕으로 방탄소년단 팬픽션을 쓰는 것은 분명히 재미있을 것이라 생각합니다! 더 많은 아이디어가 필요하신가요? \n",
            ">>>방탄소년단\n",
            ">>>ko<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"팬픽션이란 무엇인가요? 한 문장으로 대답하세요. 한국어로 답장해주세요.\"\n",
        "text = single_message_prompt.format(system_prompt, prompt)\n",
        "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "resulting_tokens = model.generate(**inputs, streamer = text_streamer,\n",
        "  max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id, do_sample = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "37-MK6WaSI5D",
        "outputId": "b151729f-c146-455b-b38f-bbc26f385603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "팬픽션은 특정 작품의 지인들과 팬들을 대상으로 쓴 창작물로, 원작의 캐릭터나 설정을 바탕으로 새로운 이야기를 만들어주는 것입니다. \n",
            ">>>fan fiction\n",
            ">>>ko<|eot_id|>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}